
It is important to note that query speed and time testing was conducted on a local copy of the database. This had a significantly different performance compared to the original database in the NCI environment. As such the recommendations are also based on theory.

--------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------
Find missing Indexes
--------------------------------------------------------------------------------------------------------------
Some were deleted with concerns about query planner optimisation/confusion. This stackoverflow answer seems to deem that concern unnecessary:
https://stackoverflow.com/questions/18507758/number-of-indexes-per-table

However extra indexes that aren't used do cost more space and minor impact on insert times.

Based on what indexes currently exist, it was possible to identify missing indexes based on their metadata_type_ref and a count of indexes for each product.

<include table identifying missing indexes>

--------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------
Fix Missing Indexes
--------------------------------------------------------------------------------------------------------------
After identifying which indexes are missing, the product per metadata_type_ref with the most indexes are used as index templates to fill the missing indexes.

Note: whether an index is gist or btree is not visible when double-clicking the index in PyCharm, but is visible in pg_indexes

See file: create_missing_indexes_script.py

<refer to previous table to identify which indexes were used as templates>

--------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------
Min/max (Extent) indexes
--------------------------------------------------------------------------------------------------------------
8 new indexes on individual corner coordinates (upper left, upper right, lower left and lower right - latitudes and longitudes) significantly improves max/min query speeds on those columns.

<include query speed tests>

The new indexes had minimal impacts on insertion times.

<include insertion speed tests>

Required <insert memory size> more hard drive space to maintain.


Depending on the requirements of dea-dashboard and datacube-ows - these are the recommended indexes to implement:
???

•	Following on from min/max
    o	Produce queries that use the min/max indexes to replace functionality in dea-dashboard
    o	Produce queries that use the min/max indexes to replace functionality in datacube-ows Probably

--------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------
Research performance (and limitations) of the current dataset spatial and temporal search query

- index appears to slow down the query in some cases
--------------------------------------------------------------------------------------------------------------
Main current limitation is the retrieval of data from the JSON followed by the least or greatest function to identify the border conditions for each row. If new columns were added to dataset that contained the precomputed values that most queries refer to it should be possible to improve query speeds.

<include table indicating query time differences>

Need to identify the reasoning for so many partial indexes. Possible that query performance would not change if including dataset_type_ref as the first column of the index but would improve insertion or creation of new product.

•	Research performance (and limitations) of the current dataset spatial and temporal search query
    o	Write up the current limitations
    o	Propose a better querying methodology
really really simple Talk to Robbie (Armstrong) and/or Ashoka

•	Talk to Jeremy about potential problems with proposed improvements He did the initial implementation of the ODC database code, and has gone on to create cubedash/dea-dashboard, so is very familiar with the problem space.

The state of the database was only meant to be temporary but followup measures were never taken. Many of the current issues were foreseen and considered non-critical. Cubedash and dea-dashboard were pilot projects as well that's why they were not incorporated into ODC.


•	Review Slow queries from database logs
    o	Are there any common things we should improve.
•	Take another snapshot of index usage and compare


*** concerns ***
the new structure was faster for the slow queries - not sure about the impact on the already fast queries
- need to test queries that aren't slow



------
Clustering
------
***NB: Locks table for duration of cluster command

Either way, CLUSTER using the multicolumn index from above can help performance:

`CLUSTER ips USING index_ips_begin_end_ip_num;`

This way, candidates fulfilling your first condition are packed onto the same or adjacent data pages. Can help performance a lot with if you have lots of rows per value of the first column. Else it is hardly effective.

https://stackoverflow.com/questions/1251636/what-do-clustered-and-non-clustered-index-actually-mean?rq=1

"""
A clustered index means you are telling the database to store close values actually close to one another on the disk. This has the benefit of rapid scan / retrieval of records falling into some range of clustered index values.

For example, you have two tables, Customer and Order:

Customer
----------
ID
Name
Address

Order
----------
ID
CustomerID
Price

If you wish to quickly retrieve all orders of one particular customer, you may wish to create a clustered index on the "CustomerID" column of the Order table. This way the records with the same CustomerID will be physically stored close to each other on disk (clustered) which speeds up their retrieval.

P.S. The index on CustomerID will obviously be not unique, so you either need to add a second field to "uniquify" the index or let the database handle that for you but that's another story.

Regarding multiple indexes. You can have only one clustered index per table because this defines how the data is physically arranged. If you wish an analogy, imagine a big room with many tables in it. You can either put these tables to form several rows or pull them all together to form a big conference table, but not both ways at the same time. A table can have other indexes, they will then point to the entries in the clustered index which in its turn will finally say where to find the actual data.
"""

https://bclennox.com/the-postgres-cluster-command







--------------------------------------------------
--------------------------------------------------
--------------------------------------------------
[2019-01-30 09:31:23] Connected
sql> EXPLAIN ANALYSE
       SELECT id
       FROM agdc.eo_1_data
       WHERE archived IS NULL
         AND dataset_type_ref = 92
         AND (agdc.float8range(lat_least, lat_greatest, '[]') &&
              '[ -31, -30)')
         AND (agdc.float8range(lon_least, lon_greatest, '[]') &&
              '[132, 133)')
[2019-01-30 09:32:34] 5 rows retrieved starting from 1 in 1 m 7 s 449 ms (execution: 1 m 7 s 28 ms, fetching: 421 ms)
sql> CREATE INDEX eo_1_dataset_type_ref_index
       ON agdc.eo_1_data
         (dataset_type_ref)
[2019-01-30 09:36:57] completed in 1 m 44 s 672 ms
sql> CREATE INDEX eo_1_archived_index
       ON agdc.eo_1_data
         (archived)
[2019-01-30 09:38:40] completed in 1 m 43 s 373 ms
sql> CREATE INDEX eo_1_dataset_type_ref_not_archived_index
       ON agdc.eo_1_data
         (dataset_type_ref)
     WHERE archived IS NULL
[2019-01-30 09:40:28] completed in 1 m 41 s 371 ms
sql> CLUSTER agdc.dataset USING agdc.tix_active_dataset_type
[2019-01-30 09:40:37] [42601] ERROR: syntax error at or near "."
[2019-01-30 09:40:37] Position: 32
sql> CLUSTER agdc.dataset USING tix_active_dataset_type
[2019-01-30 09:40:57] [0A000] ERROR: cannot cluster on partial index "tix_active_dataset_type"
sql> CREATE INDEX dataset_dataset_type_ref_index
       ON agdc.dataset
         (dataset_type_ref)
[2019-01-30 09:57:21] completed in 15 m 1 s 603 ms
sql> CLUSTER agdc.dataset USING dataset_dataset_type_ref_index
[2019-01-30 15:28:47] Cancelling...
[2019-01-30 15:28:47] [57014] ERROR: canceling statement due to user request
sql> CREATE INDEX cluster_index
       ON agdc.eo_1_data
         USING gist (archived,
                     dataset_type_ref,
                     agdc.float8range(lat_least, lat_greatest, '[]'::text),
                     agdc.float8range(lon_least, lon_greatest, '[]'::text),
                     tstzrange(from_dt, to_dt, '[]'::text))
[2019-01-30 15:29:13] Cancelling...
[2019-01-30 15:29:14] [57014] ERROR: canceling statement due to user request
sql> CREATE INDEX eo_1_data_cluster_index
       ON agdc.eo_1_data
         USING gist (archived,
                     dataset_type_ref,
                     agdc.float8range(lat_least, lat_greatest, '[]'::text),
                     agdc.float8range(lon_least, lon_greatest, '[]'::text),
                     tstzrange(from_dt, to_dt, '[]'::text))
[2019-01-30 18:28:56] completed in 2 h 59 m 27 s 423 ms
sql> CREATE INDEX dataset_archived_dataset_type_ref_index
       ON agdc.dataset
         (archived, dataset_type_ref)
[2019-01-30 18:46:23] completed in 17 m 26 s 558 ms
sql> CLUSTER agdc.dataset USING dataset_archived_dataset_type_ref_index
[2019-02-01 03:36:30] completed in 1 d 8 h 50 m 6 s 891 ms
sql> analyse agdc.dataset
[2019-02-01 03:39:32] completed in 3 m 1 s 927 ms
sql> CLUSTER agdc.eo_1_data USING eo_1_data_cluster_index
[2019-02-01 22:39:55] completed in 19 h 0 m 23 s 477 ms
sql> analyse agdc.eo_1_data
[2019-02-01 22:40:43] completed in 47 s 596 ms
